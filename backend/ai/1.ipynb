{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea34ae14-41d8-474d-b32c-ef964d147fbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11c5c699-fd9c-4c72-a4f8-43b992f8e163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌐 Enter websites to scrape, separated by commas:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "👉  https://thehackernews.com, https://krebsonsecurity.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📡 Scraping: https://thehackernews.com\n",
      "✅ Found 12 CS-related headlines.\n",
      "\n",
      "📰 The Hacker News | #1 Trusted Source for Cybersecurity News\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 China's Massistant Tool Secretly Extracts SMS, GPS Data, and Images From Confiscated Phones\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 New Webinar: Identity Attacks Have Changed — Have Your IR Playbooks?\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 Ivanti Zero-Days Exploited to Drop MDifyLoader and Launch In-Memory Cobalt Strike Attacks\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 CERT-UA Discovers LAMEHUG Malware Linked to APT28, Using LLM for Phishing Campaign\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 From Backup to Cyber Resilience: Why IT Leaders Must Rethink Backup in the Age of Ransomware\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Hackers Use GitHub Repositories to Host Amadey Malware and Data Stealers, Bypassing Filters\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 Hackers Exploit Apache HTTP Server Flaw to Deploy Linuxsys Cryptocurrency Miner\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 Europol Disrupts NoName057(16) Hacktivist Group Linked to DDoS Attacks Against Ukraine\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 CTEM vs ASM vs Vulnerability Management: What Security Leaders Need to Know in 2025\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 Chinese Hackers Target Taiwan's Semiconductor Sector with Cobalt Strike, Custom Backdoors\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Automation ≠ Autopilot: Rethinking AI in Corporate Security and Compliance\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "\n",
      "📡 Scraping: https://krebsonsecurity.com\n",
      "✅ Found 2 CS-related headlines.\n",
      "\n",
      "📰 DOGE Denizen Marko Elez Leaked API Key for xAI\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Senator Chides FBI for Weak Advice on Mobile Security\n",
      "   → Predicted: Real ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# 📌 download NLTK resources if you haven’t already\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# 🔷 Load model & tokenizer\n",
    "model = load_model(\"CS_model.h5\")\n",
    "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# 🔷 Config\n",
    "max_len = 50\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "cs_keywords = [\n",
    "    'cyber', 'hack', 'breach', 'malware', 'phishing', 'ransomware',\n",
    "    'infrastructure', 'ddos', 'security', 'data', 'attack', 'vulnerability',\n",
    "    'privacy', 'leak', 'spyware', 'exploit', 'threat', 'database'\n",
    "]\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 🔷 Input websites\n",
    "print(\"🌐 Enter websites to scrape, separated by commas:\")\n",
    "user_input = input(\"👉 \").strip()\n",
    "sites = [url.strip() for url in user_input.split(\",\") if url.strip()]\n",
    "\n",
    "if not sites:\n",
    "    print(\"⚠️ No websites entered. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "for url in sites:\n",
    "    print(f\"\\n📡 Scraping: {url}\")\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(5)\n",
    "        html = driver.page_source\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        all_headlines = [\n",
    "            h.text.strip()\n",
    "            for tag in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]\n",
    "            for h in soup.find_all(tag)\n",
    "        ]\n",
    "\n",
    "        # 🔷 Filter CS-related only\n",
    "        headlines = [h for h in all_headlines if any(kw in h.lower() for kw in cs_keywords)]\n",
    "\n",
    "        if not headlines:\n",
    "            print(\"⚠️ No CS-related headlines found.\")\n",
    "        else:\n",
    "            print(f\"✅ Found {len(headlines)} CS-related headlines.\\n\")\n",
    "\n",
    "            for text in headlines:\n",
    "                cleaned = clean_text(text)\n",
    "                seq = tokenizer.texts_to_sequences([cleaned])\n",
    "                padded = pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')\n",
    "                pred = model.predict(padded, verbose=0)\n",
    "                label = 'Real ✅' if int(pred[0][0] <= 0.5) else 'Fake 🚨'\n",
    "                print(f\"📰 {text}\")\n",
    "                print(f\"   → Predicted: {label}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error scraping {url}: {e}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae762832-313e-4dda-9f16-2b586b5264d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbb01e38-dee4-4ee6-a97a-eb8d67f35500",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌐 Enter websites to scrape, separated by commas:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "👉  https://thehackernews.com, https://krebsonsecurity.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📡 Scraping: https://thehackernews.com\n",
      "✅ Found 12 CS-related headlines.\n",
      "\n",
      "📰 The Hacker News | #1 Trusted Source for Cybersecurity News\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 China's Massistant Tool Secretly Extracts SMS, GPS Data, and Images From Confiscated Phones\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 New Webinar: Identity Attacks Have Changed — Have Your IR Playbooks?\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 Ivanti Zero-Days Exploited to Drop MDifyLoader and Launch In-Memory Cobalt Strike Attacks\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 CERT-UA Discovers LAMEHUG Malware Linked to APT28, Using LLM for Phishing Campaign\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 From Backup to Cyber Resilience: Why IT Leaders Must Rethink Backup in the Age of Ransomware\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Hackers Use GitHub Repositories to Host Amadey Malware and Data Stealers, Bypassing Filters\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 Hackers Exploit Apache HTTP Server Flaw to Deploy Linuxsys Cryptocurrency Miner\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 Europol Disrupts NoName057(16) Hacktivist Group Linked to DDoS Attacks Against Ukraine\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 CTEM vs ASM vs Vulnerability Management: What Security Leaders Need to Know in 2025\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 Chinese Hackers Target Taiwan's Semiconductor Sector with Cobalt Strike, Custom Backdoors\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Automation ≠ Autopilot: Rethinking AI in Corporate Security and Compliance\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "\n",
      "📡 Scraping: https://krebsonsecurity.com\n",
      "✅ Found 2 CS-related headlines.\n",
      "\n",
      "📰 DOGE Denizen Marko Elez Leaked API Key for xAI\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Senator Chides FBI for Weak Advice on Mobile Security\n",
      "   → Predicted: Real ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# 📌 download NLTK resources if you haven’t already\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# 🔷 Load model & tokenizer\n",
    "model = load_model(\"CS_model.h5\")\n",
    "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# 🔷 Config\n",
    "max_len = 50\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "cs_keywords = [\n",
    "    'cyber', 'hack', 'breach', 'malware', 'phishing', 'ransomware',\n",
    "    'infrastructure', 'ddos', 'security', 'data', 'attack', 'vulnerability',\n",
    "    'privacy', 'leak', 'spyware', 'exploit', 'threat', 'database'\n",
    "]\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 🔷 Input websites\n",
    "print(\"🌐 Enter websites to scrape, separated by commas:\")\n",
    "user_input = input(\"👉 \").strip()\n",
    "sites = [url.strip() for url in user_input.split(\",\") if url.strip()]\n",
    "\n",
    "if not sites:\n",
    "    print(\"⚠️ No websites entered. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# 🔷 Headless Chrome with human-like options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")  # or \"--headless\"\n",
    "options.add_argument(\"start-maximized\")\n",
    "options.add_argument(\"disable-infobars\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    \"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    ")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "for url in sites:\n",
    "    print(f\"\\n📡 Scraping: {url}\")\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(5)\n",
    "        html = driver.page_source\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        all_headlines = [\n",
    "            h.text.strip()\n",
    "            for tag in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]\n",
    "            for h in soup.find_all(tag)\n",
    "        ]\n",
    "\n",
    "        # 🔷 Filter CS-related only\n",
    "        headlines = [h for h in all_headlines if any(kw in h.lower() for kw in cs_keywords)]\n",
    "\n",
    "        if not headlines:\n",
    "            print(\"⚠️ No CS-related headlines found.\")\n",
    "        else:\n",
    "            print(f\"✅ Found {len(headlines)} CS-related headlines.\\n\")\n",
    "\n",
    "            for text in headlines:\n",
    "                cleaned = clean_text(text)\n",
    "                seq = tokenizer.texts_to_sequences([cleaned])\n",
    "                padded = pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')\n",
    "                pred = model.predict(padded, verbose=0)\n",
    "                label = 'Real ✅' if int(pred[0][0] <= 0.5) else 'Fake 🚨'\n",
    "                print(f\"📰 {text}\")\n",
    "                print(f\"   → Predicted: {label}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error scraping {url}: {e}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1c2a10-d9cf-4695-902c-3993afcf01ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fdde89d-5db1-4a35-b05d-822cf4ea907e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256173ce-06be-4cbd-96b9-c5ad64c74bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "152d7356-3dd8-45f0-8ff3-0e6243a3e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cs(user_input):\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    from bs4 import BeautifulSoup\n",
    "    import nltk\n",
    "    import re\n",
    "    import pickle\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    \n",
    "    # 📌 download NLTK resources if you haven’t already\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    \n",
    "    \n",
    "    # 🔷 Load model & tokenizer\n",
    "    model = load_model(\"CS_model.h5\")\n",
    "    with open(\"tokenizer.pkl\", \"rb\") as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    \n",
    "    # 🔷 Config\n",
    "    max_len = 50\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    cs_keywords = [\n",
    "        'cyber', 'hack', 'breach', 'malware', 'phishing', 'ransomware',\n",
    "        'infrastructure', 'ddos', 'security', 'data', 'attack', 'vulnerability',\n",
    "        'privacy', 'leak', 'spyware', 'exploit', 'threat', 'database'\n",
    "    ]\n",
    "    \n",
    "    def clean_text(text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    # 🔷 Input websites\n",
    "    # user_input = input(\"👉 \").strip()\n",
    "    sites = [url.strip() for url in user_input.split(\",\") if url.strip()]\n",
    "    \n",
    "    if not sites:\n",
    "        print(\"⚠️ No websites entered. Exiting.\")\n",
    "        exit()\n",
    "    \n",
    "    # 🔷 Headless Chrome with human-like options\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless=new\")  # or \"--headless\"\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    options.add_argument(\"disable-infobars\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "    options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    \n",
    "    for url in sites:\n",
    "        print(f\"\\n📡 Scraping: {url}\")\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            driver.implicitly_wait(5)\n",
    "            html = driver.page_source\n",
    "    \n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            all_headlines = [\n",
    "                h.text.strip()\n",
    "                for tag in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]\n",
    "                for h in soup.find_all(tag)\n",
    "            ]\n",
    "    \n",
    "            # 🔷 Filter CS-related only\n",
    "            headlines = [h for h in all_headlines if any(kw in h.lower() for kw in cs_keywords)]\n",
    "    \n",
    "            if not headlines:\n",
    "                print(\"⚠️ No CS-related headlines found.\")\n",
    "            else:\n",
    "                print(f\"✅ Found {len(headlines)} CS-related headlines.\\n\")\n",
    "    \n",
    "                for text in headlines:\n",
    "                    cleaned = clean_text(text)\n",
    "                    seq = tokenizer.texts_to_sequences([cleaned])\n",
    "                    padded = pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')\n",
    "                    pred = model.predict(padded, verbose=0)\n",
    "                    label = 'Real ✅' if int(pred[0][0] <= 0.5) else 'Fake 🚨'\n",
    "                    print(f\"📰 {text}\")\n",
    "                    print(f\"   → Predicted: {label}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error scraping {url}: {e}\")\n",
    "    \n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dce6890b-5ff7-4d38-a9aa-bf10e0f4b9fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📡 Scraping: https://www.cyberscoop.com/\n",
      "✅ Found 32 CS-related headlines.\n",
      "\n",
      "📰 Featured on CyberScoop\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 United Natural Foods loses up to $400M in sales after cyberattack\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 UK sanctions Russian hackers, spies as US weighs its own punishments for Russia\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Senate Democrats seek answers on Trump overhaul of immigrant database to find noncitizen voters\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Ryuk ransomware operator extradited to US, faces five years in federal prison\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 House hearing will use Stuxnet to search for novel ways to confront OT cyberthreats\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 SonicWall customers hit by fresh, ongoing attacks targeting fully patched SMA 100 devices\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Pro-Russian DDoS group NoName057(16) disrupted by international law enforcement operation\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Former Army soldier pleads guilty to widespread attack spree linked to AT&T, Snowflake and others\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Who needs VC funding? How cybercriminals spread their ill-gotten gains to everyday business ventures\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Trump and others want to ramp up cyber offense, but there’s plenty of doubt about the idea\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 A major cybersecurity law is expiring soon — and advocates are prepping to push Congress for renewal\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 What defenders are learning from Black Basta’s leaked chat logs\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Legal barriers complicate justice for spyware victims\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Agencies push for digital transformation amid security challenges\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Want to scale cyber defenders? Focus on AI-enabled security and organization-wide training\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Data-driven decision-making: The power of enhanced event logging\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 Why blocking cyber threats is no longer enough\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 To Catch a Thief: China’s Rise to Cyber Supremacy\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Datadog’s Brian Mikkelsen on unifying zero-trust security efforts\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 Valence Security’s Yoni Shohet on the growing risk tied to SaaS applications\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 How Full Content Inspection offers a new era of real-time cybersecurity\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 House passes bill to formalize NTIA’s cyber role following Salt Typhoon attacks\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 CitrixBleed 2 beckons sweeping alarm as exploits spread across the globe\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Why skipping security prompting on Grok’s newest model is a huge mistake\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 New White House cyber executive order pushes rules as code\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Virtru secures $50 million investment to advance data-centric security standards\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 French police arrest Russian pro basketball player on behalf of US over ransomware suspicions\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 UK arrests four for cyberattacks on major British retailers\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Trump bill will have major impact on health care cybersecurity, experts warn Congress\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Appeals court clears path for El Salvadoran journos to sue spyware maker\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Webinar: Cloud Security for Mission Resilience\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "\n",
      "📡 Scraping: https://krebsonsecurity.com/\n",
      "✅ Found 2 CS-related headlines.\n",
      "\n",
      "📰 DOGE Denizen Marko Elez Leaked API Key for xAI\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Senator Chides FBI for Weak Advice on Mobile Security\n",
      "   → Predicted: Real ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cs(\"https://www.cyberscoop.com/, https://krebsonsecurity.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "899a7ef8-7e7f-42db-8ddd-6f7e25868f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"https://thehackernews.com/\",\n",
    "#         \"https://krebsonsecurity.com/\",\n",
    "#         \"https://www.cyberscoop.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6730ed2c-e92f-4873-a6ba-a0b78be6b352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29eabd5a-3200-4cd4-80ce-8bb9c3d07458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CS(user_input):\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    from bs4 import BeautifulSoup\n",
    "    import nltk\n",
    "    import re\n",
    "    import pickle\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    # 📌 download NLTK resources if you haven’t already\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    \n",
    "    \n",
    "    # 🔷 Load model & tokenizer\n",
    "    model = load_model(\"CS_model.h5\")\n",
    "    with open(\"tokenizer.pkl\", \"rb\") as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    \n",
    "    # 🔷 Config\n",
    "    max_len = 50\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    cs_keywords = [\n",
    "        'cyber', 'hack', 'breach', 'malware', 'phishing', 'ransomware',\n",
    "        'infrastructure', 'ddos', 'security', 'data', 'attack', 'vulnerability',\n",
    "        'privacy', 'leak', 'spyware', 'exploit', 'threat', 'database'\n",
    "    ]\n",
    "    \n",
    "    def clean_text(text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    # 🔷 Input websites\n",
    "    # print(\"🌐 Enter websites to scrape, separated by commas:\")\n",
    "    # user_input = input(\"👉 \").strip()\n",
    "    sites = [url.strip() for url in user_input.split(\",\") if url.strip()]\n",
    "    \n",
    "    if not sites:\n",
    "        print(\"⚠️ No websites entered. Exiting.\")\n",
    "        exit()\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    \n",
    "    for url in sites:\n",
    "        print(f\"\\n📡 Scraping: {url}\")\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            driver.implicitly_wait(5)\n",
    "            html = driver.page_source\n",
    "    \n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            all_headlines = [\n",
    "                h.text.strip()\n",
    "                for tag in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]\n",
    "                for h in soup.find_all(tag)\n",
    "            ]\n",
    "    \n",
    "            # 🔷 Filter CS-related only\n",
    "            headlines = [h for h in all_headlines if any(kw in h.lower() for kw in cs_keywords)]\n",
    "    \n",
    "            if not headlines:\n",
    "                print(\"⚠️ No CS-related headlines found.\")\n",
    "            else:\n",
    "                print(f\"✅ Found {len(headlines)} CS-related headlines.\\n\")\n",
    "    \n",
    "                for text in headlines:\n",
    "                    cleaned = clean_text(text)\n",
    "                    seq = tokenizer.texts_to_sequences([cleaned])\n",
    "                    padded = pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')\n",
    "                    pred = model.predict(padded, verbose=0)\n",
    "                    label = 'Real ✅' if int(pred[0][0] <= 0.5) else 'Fake 🚨'\n",
    "                    print(f\"📰 {text}\")\n",
    "                    print(f\"   → Predicted: {label}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error scraping {url}: {e}\")\n",
    "    \n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eae38d14-55f8-4a5e-88b2-24e7044b5388",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌐 Enter websites to scrape, separated by commas:\n",
      "\n",
      "📡 Scraping: https://www.cyberscoop.com/\n",
      "✅ Found 32 CS-related headlines.\n",
      "\n",
      "📰 Featured on CyberScoop\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 United Natural Foods loses up to $400M in sales after cyberattack\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 UK sanctions Russian hackers, spies as US weighs its own punishments for Russia\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Senate Democrats seek answers on Trump overhaul of immigrant database to find noncitizen voters\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Ryuk ransomware operator extradited to US, faces five years in federal prison\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 House hearing will use Stuxnet to search for novel ways to confront OT cyberthreats\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 SonicWall customers hit by fresh, ongoing attacks targeting fully patched SMA 100 devices\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Pro-Russian DDoS group NoName057(16) disrupted by international law enforcement operation\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Former Army soldier pleads guilty to widespread attack spree linked to AT&T, Snowflake and others\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Who needs VC funding? How cybercriminals spread their ill-gotten gains to everyday business ventures\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Trump and others want to ramp up cyber offense, but there’s plenty of doubt about the idea\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 A major cybersecurity law is expiring soon — and advocates are prepping to push Congress for renewal\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 What defenders are learning from Black Basta’s leaked chat logs\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Legal barriers complicate justice for spyware victims\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Agencies push for digital transformation amid security challenges\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Want to scale cyber defenders? Focus on AI-enabled security and organization-wide training\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Data-driven decision-making: The power of enhanced event logging\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 Why blocking cyber threats is no longer enough\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 To Catch a Thief: China’s Rise to Cyber Supremacy\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Datadog’s Brian Mikkelsen on unifying zero-trust security efforts\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 Valence Security’s Yoni Shohet on the growing risk tied to SaaS applications\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 How Full Content Inspection offers a new era of real-time cybersecurity\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 House passes bill to formalize NTIA’s cyber role following Salt Typhoon attacks\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 CitrixBleed 2 beckons sweeping alarm as exploits spread across the globe\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Why skipping security prompting on Grok’s newest model is a huge mistake\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 New White House cyber executive order pushes rules as code\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Virtru secures $50 million investment to advance data-centric security standards\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 French police arrest Russian pro basketball player on behalf of US over ransomware suspicions\n",
      "   → Predicted: Fake 🚨\n",
      "\n",
      "📰 UK arrests four for cyberattacks on major British retailers\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Trump bill will have major impact on health care cybersecurity, experts warn Congress\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Appeals court clears path for El Salvadoran journos to sue spyware maker\n",
      "   → Predicted: Real ✅\n",
      "\n",
      "📰 Webinar: Cloud Security for Mission Resilience\n",
      "   → Predicted: Real ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CS(\"https://www.cyberscoop.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f13e4f-9a7a-4a4a-bbc0-99db2f686446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
